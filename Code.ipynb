{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"C:/Users/pranav/Desktop/DOC's/0000950170-98-000413.txt\")\n",
    "raw = f.read().lower()\n",
    "print(len(raw))\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa4190161e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(raw)\n",
    "# print(tokenized_text)\n",
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(raw)\n",
    "# print(tokenized_word)\n",
    "len(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for most_common words.\n",
    "# fdist.most_common() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'above', 'after', 'again', 'all', 'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', 'should', 'so', 'some', 'such', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "StopWords_Generic = r\"C:\\Users\\pranav\\Desktop\\Data Science\\SEC Filing Text Analysis\\StopWords_Generic.txt\"\n",
    "with open(StopWords_Generic,'r') as stop:\n",
    "    stopwords = stop.read().lower()\n",
    "    stopwords= swords.split('\\n')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203758\n",
      "132815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "132815"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "for w in tokenized_word:\n",
    "    if w not in stopwords:\n",
    "        filtered_sent.append(w)\n",
    "print(len(tokenized_word))\n",
    "# print(filtered_sent)\n",
    "print(len(filtered_sent))\n",
    "len(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n",
      "2356\n"
     ]
    }
   ],
   "source": [
    "Positivefile= r\"C:\\Users\\pranav\\Desktop\\Data Science\\SEC Filing Text Analysis\\Positive_words.txt\"\n",
    "with open(Positivefile,'r') as Positivefile:\n",
    "    positive = Positivefile.read().lower()\n",
    "positivewords= positive.split('\\n')\n",
    "print(len(positivewords))\n",
    "Negativefile= r\"C:\\Users\\pranav\\Desktop\\Data Science\\SEC Filing Text Analysis\\Negative_word.txt\"\n",
    "with open(Negativefile,'r') as Negativefile:\n",
    "    negative = Negativefile.read().lower()\n",
    "negativewords= negative.split('\\n')\n",
    "print(len(negativewords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n"
     ]
    }
   ],
   "source": [
    "positive_no=0\n",
    "for w in filtered_sent:\n",
    "    if w in positivewords:\n",
    "        positive_no +=1\n",
    "p_sum = positive_no\n",
    "print(p_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989\n"
     ]
    }
   ],
   "source": [
    "negative_no=0\n",
    "for w in filtered_sent:\n",
    "    if w in negativewords:\n",
    "        negative_no -=1\n",
    "n_sum = negative_no * -1\n",
    "print(n_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4764139292476133\n"
     ]
    }
   ],
   "source": [
    "Polarity_Score = (p_sum - n_sum)/((p_sum + n_sum) +  0.000001)\n",
    "print(Polarity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030486014380676232\n"
     ]
    }
   ],
   "source": [
    "Subjectivity_Score = (p_sum + n_sum)/ ((len(filtered_sent)) + 0.000001)\n",
    "print(Subjectivity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Negative: Polarity Score below -0.5\n",
    "\n",
    "# Negative: Polarity Score between -0.5 and 0\n",
    "\n",
    "# Neutral: Polarity Score equal to 0\n",
    "\n",
    "# Positive: Polarity Score between 0 and 0.5\n",
    "\n",
    "#Very Positive: Polarity Score above 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "def sentimental_score(Polarity_Score):\n",
    "    if Polarity_Score < -0.5:\n",
    "        print(\"Most Negative\")\n",
    "    elif Polarity_Score > -0.5 and Polarity_Score < 0:\n",
    "        print(\"Negative\")\n",
    "    elif Polarity_Score == 0.0:\n",
    "        print(\"Neutral\")\n",
    "    elif Polarity_Score > 0 and Polarity_Score < 0.5:\n",
    "        print(\"Positive\")\n",
    "    elif Polarity_Score > 0.5 :\n",
    "        print(\"Very Positive\")\n",
    "sentimental_score(Polarity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Sentence Length = the number of words / the number of sentences\n",
    "# Percentage of Complex words = the number of complex words / the number of words \n",
    "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=len(tokenized_word)\n",
    "# b=len(tokenized_text)\n",
    "def avg_sentence_len(a,b):\n",
    "    a=len(tokenized_word)\n",
    "    b=len(tokenized_text)\n",
    "    avgseclen= a/b\n",
    "    return avgseclen\n",
    "avg_sentence_len(a,b)\n",
    "# x = avg_sentence_len(a,b)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15014379803492378"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def per_complex_word(tokenized_word):\n",
    "    complex_words=0\n",
    "    sop = tokenized_word\n",
    "    per_complex=0\n",
    "    for word in sop:\n",
    "        vowels=0\n",
    "        if word.endswith(('s','ies','es','ed','ing','ly')):\n",
    "#             print(\"0\")\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels+=1\n",
    "#                     print(vowels)\n",
    "            if(vowels > 2):\n",
    "                complex_words+=1\n",
    "#     return complex_words\n",
    "    if len(sop) !=0:\n",
    "        per_complex=complex_words/len(sop)\n",
    "    return per_complex     \n",
    "per_complex_word(tokenized_word)\n",
    "# print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.92629470808659"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fog_index():\n",
    "    x= avg_sentence_len(a,b)\n",
    "    y= per_complex_word(tokenized_word)\n",
    "    fog= 0.4 * (x+y)\n",
    "    return fog\n",
    "fog_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30593"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complex_word_count(tokenized_word):\n",
    "    complex_words=0\n",
    "    sop = tokenized_word\n",
    "    per_complex=0\n",
    "    for word in sop:\n",
    "        vowels=0\n",
    "        if word.endswith(('s','ies','es','ed','ing','ly')):\n",
    "#             print(\"0\")\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels+=1\n",
    "#                     print(vowels)\n",
    "            if(vowels > 2):\n",
    "                complex_words+=1\n",
    "    return complex_words\n",
    "complex_word_count(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203758"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(tokenized_word):\n",
    "    count =len(tokenized_word)\n",
    "    return count\n",
    "word_count(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1487"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraining_dictionary = r\"C:\\Users\\pranav\\Desktop\\Data Science\\SEC Filing Text Analysis\\constraining_dictionary.txt\"\n",
    "with open(constraining_dictionary ,'r') as constrain:\n",
    "    constrain_Dict=constrain.read().lower()\n",
    "constrainDict = constrain_Dict.split('\\n')\n",
    "def constrain_words(tokenized_word):\n",
    "    contrains=0\n",
    "    for word in tokenized_word:\n",
    "        if word in constrainDict:\n",
    "            contrains +=1\n",
    "    return contrains\n",
    "constrain_words(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_dictionary = r\"C:\\Users\\pranav\\Desktop\\Data Science\\SEC Filing Text Analysis\\uncertainty_dictionary.txt\"\n",
    "with open(uncertainty_dictionary ,'r') as uncertain:\n",
    "    uncertain_Dict=uncertain.read().lower()\n",
    "uncertainDict = uncertain_Dict.split('\\n')\n",
    "def uncertain_words(tokenized_word):\n",
    "    uncertain=0\n",
    "    for word in tokenized_word:\n",
    "        if word in uncertainDict:\n",
    "            uncertain +=1\n",
    "    return uncertain\n",
    "uncertain_words(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
